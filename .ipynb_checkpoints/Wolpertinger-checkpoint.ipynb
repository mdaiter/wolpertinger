{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371c9b26-eb59-423b-86cb-38ea1906db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tinygrad\n",
    "from tinygrad import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb2cba9-e51e-4282-a8bb-154dc387970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None) -> Tensor:\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return Tensor.uniform(*size, low=float(-v), high=float(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d0282f-dd59-44e1-9380-a648fceb5943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (256, 3) float (<BinaryOps.ADD: 1>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanin_init((256,3), 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e42650-0141-4cf3-bf21-5c0c7158d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn, Tensor, dtypes\n",
    "class Actor():\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=256, hidden2=128, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, nb_actions)\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight = fanin_init(self.fc1.weight.size())\n",
    "        self.fc2.weight = fanin_init(self.fc2.weight.size())\n",
    "        self.fc3.weight = Tensor.uniform(\n",
    "            *(self.fc3.weight.size()), \n",
    "            low=float(-init_w), high=float(init_w)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        out = self.fc1(x.cast(dtypes.float)).relu()\n",
    "        out = self.fc2(out).relu()\n",
    "        out = self.fc3(out).softsign()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3984d20-bc80-46f4-ac7d-401591299a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (3,) float (<BinaryOps.MUL: 2>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(3, 3)\n",
    "actor(Tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91797ba-d74e-47ea-bac2-992afb5ef71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=319, hidden2=128, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1+nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight = fanin_init(self.fc1.weight.size())\n",
    "        self.fc2.weight = fanin_init(self.fc2.weight.size())\n",
    "        self.fc3.weight = Tensor.uniform(\n",
    "            *(self.fc3.weight.size()),\n",
    "            low=float(-init_w), high=float(init_w)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, xs:Tensor) -> Tensor:\n",
    "        x, a = xs\n",
    "        #print(f'Critic, __call__ x: {x}, a: {a}')\n",
    "        x = x.cast(dtypes.float)\n",
    "        a = a.cast(dtypes.float)\n",
    "        out = self.fc1(x).relu()\n",
    "        # concatenate along columns\n",
    "        #print(f'len(a.shape)-1: {len(a.shape)-1}')\n",
    "        #print(f'out.shape: {out.shape}')\n",
    "        #print(f'a.shape: {a.shape}')\n",
    "        c_in = out.cat(a, dim=len(a.shape)-1)\n",
    "        out = self.fc2(c_in).relu()\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc16e742-863e-4ad5-87ba-fdfefb4401ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m critic \u001b[38;5;241m=\u001b[39m \u001b[43mCritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m critic(Tensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m]]))\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mCritic.__init__\u001b[0;34m(self, nb_states, nb_actions, hidden1, hidden2, init_w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden1\u001b[38;5;241m+\u001b[39mnb_actions, hidden2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden2, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_w\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mCritic.init_weights\u001b[0;34m(self, init_w)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, init_w):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m     14\u001b[0m         low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m-\u001b[39minit_w), high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(init_w)\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "critic = Critic(2,2)\n",
    "critic(Tensor([[1,2], [2,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc140912-826f-4a4b-9195-5f5c2c9d39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MARK - Memory. Should be in a \"memory.py\" file\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py\n",
    "\n",
    "# This is to be understood as a transition: Given `state0`, performing `action`\n",
    "# yields `reward` and results in `state1`, which might be `terminal`.\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "def sample_batch_indexes(low, high, size):\n",
    "    if high - low >= size:\n",
    "        # We have enough data. Draw without replacement, that is each index is unique in the\n",
    "        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n",
    "        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n",
    "        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        # Not enough data. Help ourselves with sampling from the range, but the same index\n",
    "        # can occur multiple times. This is not good and should be avoided by picking a\n",
    "        # large enough warm-up phase.\n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        # batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "        batch_idxs = np.random.randint(low, high, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        assert isinstance(v, np.ndarray) or isinstance(v, float) or isinstance(v, bool), \"v_type:{}\".format(type(v))\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "\n",
    "def zeroed_observation(observation):\n",
    "    if hasattr(observation, 'shape'):\n",
    "        return np.zeros(observation.shape)\n",
    "    elif hasattr(observation, '__iter__'):\n",
    "        out = []\n",
    "        for x in observation:\n",
    "            out.append(zeroed_observation(x))\n",
    "        return out\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, window_length, ignore_episode_boundaries=False):\n",
    "        self.window_length = window_length\n",
    "        self.ignore_episode_boundaries = ignore_episode_boundaries\n",
    "\n",
    "        self.recent_observations = deque(maxlen=window_length)\n",
    "        self.recent_terminals = deque(maxlen=window_length)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_terminals.append(terminal)\n",
    "\n",
    "    def get_recent_state(self, current_observation):\n",
    "        # This code is slightly complicated by the fact that subsequent observations might be\n",
    "        # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "        # This is probably not that important in practice but it seems cleaner.\n",
    "        state = [current_observation]\n",
    "        idx = len(self.recent_observations) - 1\n",
    "        for offset in range(0, self.window_length - 1):\n",
    "            current_idx = idx - offset\n",
    "            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n",
    "            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                # The previously handled observation was terminal, don't add the current one.\n",
    "                # Otherwise we would leak into a different episode.\n",
    "                break\n",
    "            state.insert(0, self.recent_observations[current_idx])\n",
    "        while len(state) < self.window_length:\n",
    "            state.insert(0, zeroed_observation(state[0]))\n",
    "        return state\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'window_length': self.window_length,\n",
    "            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "class SequentialMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(SequentialMemory, self).__init__(**kwargs)\n",
    "        \n",
    "        self.limit = limit\n",
    "\n",
    "        # Do not use deque to implement the memory. This data structure may seem convenient but\n",
    "        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            # Draw random indexes such that we have at least a single entry before each\n",
    "            # index.\n",
    "            assert self.nb_entries >= 2\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n",
    "        batch_idxs = np.array(batch_idxs) + 1\n",
    "        assert np.min(batch_idxs) >= 1\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        # Create experiences\n",
    "        experiences = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            while terminal0:\n",
    "                # Skip this transition because the environment was reset here. Select a new, random\n",
    "                # transition and use this instead. This may cause the batch to contain the same\n",
    "                # transition twice.\n",
    "                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            assert 1 <= idx < self.nb_entries\n",
    "\n",
    "            # This code is slightly complicated by the fact that subsequent observations might be\n",
    "            # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "            # This is probably not that important in practice but it seems cleaner.\n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "\n",
    "            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n",
    "            # to the right. Again, we need to be careful to not include an observation from the next\n",
    "            # episode if the last state is terminal.\n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "        return experiences\n",
    "\n",
    "    def sample_and_split(self, batch_size, batch_idxs=None):\n",
    "        experiences = self.sample(batch_size, batch_idxs)\n",
    "\n",
    "        state0_batch = []\n",
    "        reward_batch = []\n",
    "        action_batch = []\n",
    "        terminal1_batch = []\n",
    "        state1_batch = []\n",
    "        for e in experiences:\n",
    "            state0_batch.append(e.state0)\n",
    "            state1_batch.append(e.state1)\n",
    "            reward_batch.append(e.reward)\n",
    "            action_batch.append(e.action)\n",
    "            terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "        # Prepare and validate parameters.\n",
    "        state0_batch = np.array(state0_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        state1_batch = np.array(state1_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        reward_batch = np.array(reward_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        #print(f'action_batch: {action_batch}')\n",
    "        action_batch = np.array(action_batch, dtype=\"object\").flatten().reshape(batch_size,-1).astype(np.float32)\n",
    "\n",
    "        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n",
    "\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        \n",
    "        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n",
    "        # and weather the next state is `terminal` or not.\n",
    "        if training:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "\n",
    "\n",
    "class EpisodeParameterMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(EpisodeParameterMemory, self).__init__(**kwargs)\n",
    "        self.limit = limit\n",
    "\n",
    "        self.params = RingBuffer(limit)\n",
    "        self.intermediate_rewards = []\n",
    "        self.total_rewards = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        batch_params = []\n",
    "        batch_total_rewards = []\n",
    "        for idx in batch_idxs:\n",
    "            batch_params.append(self.params[idx])\n",
    "            batch_total_rewards.append(self.total_rewards[idx])\n",
    "        return batch_params, batch_total_rewards\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.intermediate_rewards.append(reward)\n",
    "\n",
    "    def finalize_episode(self, params):\n",
    "        total_reward = sum(self.intermediate_rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.params.append(params)\n",
    "        self.intermediate_rewards = []\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.total_rewards)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6122dbb9-993f-4c44-ba7c-e8f08e3f92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312e7756-304f-4a9f-9d80-012663406775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn, Tensor, dtypes, TinyJit\n",
    "from tinygrad.nn.optim import LAMB\n",
    "\n",
    "# might have to change this to \"load_state_dict\" and manually updating state_dict\n",
    "def hard_update(target, source):\n",
    "    for target_tensor, tensor in zip(nn.state.get_parameters(target), nn.state.get_parameters(source)):\n",
    "        tensor.requires_grad = False\n",
    "        target_tensor.replace(tensor)\n",
    "        tensor.requires_grad = True\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, args, nb_states, nb_actions):\n",
    "        if args.seed > 0:\n",
    "            self.seed(args.seed)\n",
    "\n",
    "        self.nb_states =  nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        net_cfg = {\n",
    "            'hidden1': args.hidden1,\n",
    "            'hidden2': args.hidden2,\n",
    "            'init_w': args.init_w\n",
    "        }\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        print(f'Initialized DDPG with actor parameters: {len(nn.state.get_parameters(self.actor))}, lr={args.p_lr}')\n",
    "        print(f'Initialized DDPG with critic parameters: {len(nn.state.get_parameters(self.critic))}, lr={args.c_lr}')\n",
    "        \n",
    "        self.actor_optim = LAMB(params=[self.actor.fc1.weight, self.actor.fc2.weight, self.actor.fc3.weight], lr=args.p_lr, weight_decay=args.weight_decay, adam=True)\n",
    "        self.critic_optim = LAMB(params=[self.critic.fc1.weight, self.critic.fc2.weight, self.critic.fc3.weight], lr=args.c_lr, weight_decay=args.weight_decay, adam=True)\n",
    "        \n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=self.nb_actions,\n",
    "                                                       theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args.bsize\n",
    "        self.tau_update = args.tau_update\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        # Linear decay rate of exploration policy\n",
    "        self.depsilon = 1.0 / args.epsilon\n",
    "        # initial exploration rate\n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        self.continious_action_space = False\n",
    "\n",
    "    def update_policy(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            # print(f'typese of memory: s_t: {self.s_t}, a_t: {self.a_t}')\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1., 1., self.nb_actions)\n",
    "        # self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        # proto action\n",
    "        if type(s_t) is tuple and s_t[1] == {}:\n",
    "            s_t = s_t[0]\n",
    "        #print(f's_t: {s_t}')\n",
    "        orig_tensor = Tensor([np.array(list(s_t), dtype=np.float32)], dtype=dtypes.float, requires_grad=False)\n",
    "        action = self.actor(orig_tensor).numpy().squeeze(0)\n",
    "        action += self.is_training * max(self.epsilon, 0) * self.random_process.sample()\n",
    "        action = np.clip(action, -1., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        \n",
    "        # self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def reset(self, s_t):\n",
    "        self.s_t = s_t\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, dir):\n",
    "        if dir is None: return\n",
    "\n",
    "        # load all tensors to CPU\n",
    "        ml = lambda storage, loc: storage\n",
    "\n",
    "        state_dict_actor = nn.state.safe_load(\n",
    "            'output/{}/actor.safetensors'.format(dir)\n",
    "        )\n",
    "        nn.state.load_state_dict(\n",
    "            self.actor, state_dict_actor\n",
    "        )\n",
    "\n",
    "        state_dict_critic = nn.state.safe_load(\n",
    "            'output/{}/critic.safetensors'.format(dir)\n",
    "        )\n",
    "        nn.state.load_state_dict(\n",
    "            self.critic, state_dict_critic\n",
    "        )\n",
    "        print('model weights loaded')\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        state_dict_actor = nn.state.get_state_dict(self.actor)\n",
    "        nn.state.safe_save(\n",
    "            state_dict_actor,\n",
    "            '{}/actor.safetensors'.format(output)\n",
    "        )\n",
    "\n",
    "        state_dict_critic = nn.state.get_state_dict(self.critic)\n",
    "        nn.state.safe_save(\n",
    "            state_dict_critic,\n",
    "            '{}/critic.safetensors'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,seed):\n",
    "        Tensor.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84441f96-d035-40d0-8aec-27d3be5e9130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing flannlib from: /Users/msd/Code/wolpertinger/pyflann/bindings/darwin/libflann.dylib\n",
      "Importing flannlib from: /Users/msd/Code/wolpertinger/pyflann/darwin/libflann.dylib\n",
      "Importing flannlib from: /Users/msd/Code/wolpertinger/darwin/libflann.dylib\n",
      "Importing flannlib from: /Users/msd/Code/darwin/libflann.dylib\n",
      "Importing flannlib from: /Users/msd/darwin/libflann.dylib\n",
      "Importing flannlib from: /Users/darwin/libflann.dylib\n",
      "Importing flannlib from: /darwin/libflann.dylib\n"
     ]
    }
   ],
   "source": [
    "# action_space.py\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import itertools\n",
    "import pyflann\n",
    "\n",
    "\"\"\"\n",
    "    This class represents a n-dimensional unit cube with a specific number of points embeded.\n",
    "    Points are distributed uniformly in the initialization. A search can be made using the\n",
    "    search_point function that returns the k (given) nearest neighbors of the input point.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Space:\n",
    "\n",
    "    def __init__(self, low, high, points):\n",
    "\n",
    "        self._low = np.array(low)\n",
    "        self._high = np.array(high)\n",
    "        self._range = self._high - self._low\n",
    "        self._dimensions = len(low)\n",
    "        self._space_low = -1\n",
    "        self._space_high = 1\n",
    "        self._k = (self._space_high - self._space_low) / self._range\n",
    "        self.__space = init_uniform_space([self._space_low] * self._dimensions,\n",
    "                                          [self._space_high] * self._dimensions,\n",
    "                                          points)\n",
    "        self._flann = pyflann.FLANN()\n",
    "        self.rebuild_flann()\n",
    "\n",
    "    def rebuild_flann(self):\n",
    "        self._index = self._flann.build_index(self.__space, algorithm='kdtree')\n",
    "\n",
    "    def search_point(self, point, k):\n",
    "        p_in = point\n",
    "        if not isinstance(point, np.ndarray):\n",
    "            p_in = np.array([p_in])\n",
    "        p_in = p_in.astype(np.float64)\n",
    "        # p_in = self.import_point(point)\n",
    "        search_res, _ = self._flann.nn_index(p_in, k)\n",
    "        knns = self.__space[search_res]\n",
    "        p_out = []\n",
    "        for p in knns:\n",
    "            p_out.append(self.export_point(p))\n",
    "\n",
    "        if k == 1:\n",
    "            p_out = [p_out]\n",
    "        return knns, np.array(p_out)\n",
    "\n",
    "    def import_point(self, point):\n",
    "        return self._space_low + self._k * (point - self._low)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return self._low + (point - self._space_low) / self._k\n",
    "\n",
    "    def get_space(self):\n",
    "        return self.__space\n",
    "\n",
    "    def shape(self):\n",
    "        return self.__space.shape\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.shape()[0]\n",
    "\n",
    "\n",
    "class Discrete_space(Space):\n",
    "    \"\"\"\n",
    "        Discrete action space with n actions (the integers in the range [0, n))\n",
    "        1, 2, ..., n-1, n\n",
    "\n",
    "        In gym: 'Discrete' object has no attribute 'high'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):  # n: the number of the discrete actions\n",
    "        super().__init__([0], [n-1], n)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return np.round(super().export_point(point)).astype(int)\n",
    "\n",
    "\n",
    "def init_uniform_space(low, high, points):\n",
    "    dims = len(low)\n",
    "    # In Discrete situation, the action space is an one dimensional space, i.e., one row\n",
    "    points_in_each_axis = round(points**(1 / dims))\n",
    "\n",
    "    axis = []\n",
    "    for i in range(dims):\n",
    "        axis.append(list(np.linspace(low[i], high[i], points_in_each_axis)))\n",
    "\n",
    "    space = []\n",
    "    for _ in itertools.product(*axis):\n",
    "        space.append(list(_))\n",
    "\n",
    "    # space: e.g., [[1], [2], ... ,[n-1]]\n",
    "    return np.array(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd282c18-df7c-4121-a382-16e0828fde02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [[[1.     ]\n",
      "  [0.99999]\n",
      "  [0.99998]\n",
      "  [0.99997]]]\n",
      "(1, 4, 1)\n",
      "output_2: [[[2.     ]\n",
      "  [1.99998]\n",
      "  [1.99996]\n",
      "  [1.99994]]]\n",
      "(1, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "ds = Space([-2], [2], 200000)\n",
    "output, output_2 = ds.search_point(1.4123456765373, 4)\n",
    "print(f'output: {output}')\n",
    "print(output.shape)\n",
    "print(f'output_2: {output_2}')\n",
    "print(output_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c8a305-0ea3-4f6a-9310-d339bfaff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import TinyJit, Tensor\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "# see note above for hard_update\n",
    "def soft_update(target, source, tau_update):\n",
    "    for target_tensor, tensor in zip(nn.state.get_parameters(target), nn.state.get_parameters(source)):\n",
    "        tensor.requires_grad = False\n",
    "        target_tensor.replace(target_tensor * (1.0 - tau_update) + tensor * tau_update)\n",
    "        tensor.requires_grad = True\n",
    "\n",
    "def criterion(input: Tensor, target: Tensor) -> Tensor:\n",
    "    return ((target-input).pow(2)).mean()\n",
    "\n",
    "class WolpertingerAgent(DDPG):\n",
    "\n",
    "    def __init__(self, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio=0.1):\n",
    "        super().__init__(args, nb_states, nb_actions)\n",
    "        self.experiment = args.id\n",
    "        # according to the papers, it can be scaled to hundreds of millions\n",
    "        if continuous:\n",
    "            self.action_space = Space(action_low, action_high, args.max_actions)\n",
    "            self.k_nearest_neighbors = max(1, int(args.max_actions * k_ratio))\n",
    "        else:\n",
    "            self.action_space = Discrete_space(max_actions)\n",
    "            self.k_nearest_neighbors = max(1, int(max_actions * k_ratio))\n",
    "\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'Wolp3_{}k{}_{}'.format(self.action_space.get_number_of_actions(),\n",
    "                                       self.k_nearest_neighbors, self.experiment)\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space\n",
    "\n",
    "    def wolp_action(self, s_t, proto_action):\n",
    "        # get the proto_action's k nearest neighbors\n",
    "        raw_actions, actions = self.action_space.search_point(proto_action, self.k_nearest_neighbors)\n",
    "        raw_actions = raw_actions.astype(np.float32)\n",
    "        actions = actions.astype(np.float32)\n",
    "        \n",
    "        if not isinstance(s_t, np.ndarray):\n",
    "            if isinstance(s_t, tuple):\n",
    "                # print(f'Tuple detected for s_t: {s_t}')\n",
    "                s_t = np.array(s_t[0]).astype(np.float32)\n",
    "            else:\n",
    "                s_t = s_t.numpy().astype(np.float32)\n",
    "        # make all the state, action pairs for the critic\n",
    "        s_t = np.tile(s_t, [raw_actions.shape[1], 1])\n",
    "\n",
    "        s_t = s_t.reshape(len(raw_actions), raw_actions.shape[1], s_t.shape[1]) if self.k_nearest_neighbors > 1 \\\n",
    "            else s_t.reshape(raw_actions.shape[0], s_t.shape[1])\n",
    "        raw_actions = Tensor(raw_actions, dtype=dtypes.float, requires_grad=False)\n",
    "        s_t = Tensor(s_t, dtype=dtypes.float, requires_grad=False)\n",
    "\n",
    "        # evaluate each pair through the critic\n",
    "        actions_evaluation = self.critic([s_t, raw_actions])\n",
    "\n",
    "        # find the index of the pair with the maximum value\n",
    "        max_index = np.argmax(actions_evaluation.numpy(), axis=1)# 0)[0].repeat(len(actions_evaluation.numpy().flatten()))\n",
    "        max_index = max_index.reshape(len(max_index),)\n",
    "        \n",
    "        raw_actions = raw_actions.numpy().astype(np.float32)\n",
    "        # return the best action, i.e., wolpertinger action from the full wolpertinger policy\n",
    "        if self.k_nearest_neighbors > 1:\n",
    "            #print(f'raw_actions: {raw_actions}')\n",
    "            #print(f'actions: {actions}')\n",
    "            #print(f'raw_actions after mod: {raw_actions[[i for i in range(len(raw_actions))], max_index, [0]]}')\n",
    "            #print(f'raw_actions.shape: {raw_actions.shape}')\n",
    "            #print(f'max_index: {max_index}')\n",
    "            return raw_actions[[i for i in range(len(raw_actions))], max_index, [0]].reshape(len(raw_actions),1), \\\n",
    "                   actions[[i for i in range(len(actions))], max_index, [0]].reshape(len(actions),1)\n",
    "        else:\n",
    "            return raw_actions[max_index], actions[max_index]\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        # taking a continuous action from the actor\n",
    "        proto_action = super().select_action(s_t, decay_epsilon)\n",
    "        #print(f'proto_action: {proto_action}')\n",
    "\n",
    "        #print(f'select_action types: {type(s_t.dtype)}, {type(proto_action.dtype)}')\n",
    "        #if type(s_t) is tuple and s_t[1] == {}:\n",
    "        #    s_t = s_t[0]\n",
    "        raw_wolp_action, wolp_action = self.wolp_action(s_t, proto_action)\n",
    "        assert isinstance(raw_wolp_action, np.ndarray)\n",
    "        self.a_t = raw_wolp_action\n",
    "        # return the best neighbor of the proto action, this is an action for env step\n",
    "        return wolp_action[0]  # [i]\n",
    "\n",
    "    def random_action(self):\n",
    "        proto_action = super().random_action()\n",
    "        raw_action, action = self.action_space.search_point(proto_action, 1)\n",
    "        raw_action = raw_action.astype(np.float32)\n",
    "        action = action.astype(np.float32)\n",
    "        \n",
    "        raw_action = raw_action[0]\n",
    "        action = action[0]\n",
    "        assert isinstance(raw_action, np.ndarray)\n",
    "        self.a_t = raw_action\n",
    "        return action[0] # [i]\n",
    "\n",
    "    def select_target_action(self, s_t):\n",
    "        proto_action = self.actor_target(s_t)\n",
    "        #print(f'select_target_action, proto_action: {proto_action}')\n",
    "        proto_action = proto_action.clamp(-1.0, 1.0).numpy().astype(np.float32)\n",
    "        #print(f'select_target_action, proto_action clamped: {proto_action}')\n",
    "        #if type(s_t) is tuple and s_t[1] == {}:\n",
    "        #    s_t = s_t[0]\n",
    "        #print(f'select_target_action, s_t: {s_t}')\n",
    "        raw_wolp_action, wolp_action = self.wolp_action(s_t, proto_action)\n",
    "        #print(f'select_target_action, raw_wolp_action: {raw_wolp_action}')\n",
    "        return raw_wolp_action\n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        with Tensor.train():\n",
    "            state_batch, action_batch, reward_batch, \\\n",
    "            next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "    \n",
    "            # Prepare for the target q batch\n",
    "            # the operation below of critic_target does not require backward_P\n",
    "            next_state_batch = Tensor(next_state_batch, dtype=dtypes.float, requires_grad=False)\n",
    "            next_wolp_action_batch = self.select_target_action(next_state_batch)\n",
    "            #print(f'next_state_batch: {next_state_batch}')\n",
    "            #print(f'next_wolp_action_batch: {next_wolp_action_batch}')\n",
    "            next_q_values = self.critic_target((\n",
    "                next_state_batch,\n",
    "                Tensor(next_wolp_action_batch, dtype=dtypes.float, requires_grad=False)\n",
    "            ))\n",
    "            # but it requires bp in computing gradient of critic loss\n",
    "            # next_q_values.volatile = False\n",
    "    \n",
    "            # next_q_values = 0 if is terminal states\n",
    "            target_q_batch = Tensor(reward_batch, dtype=dtypes.float, requires_grad=False) + \\\n",
    "                             self.gamma * \\\n",
    "                             Tensor(terminal_batch.astype(np.float32), dtype=dtypes.float, requires_grad=False) * \\\n",
    "                             next_q_values\n",
    "\n",
    "            # Critic update\n",
    "            state_batch = Tensor(state_batch, dtype=dtypes.float, requires_grad=False)\n",
    "            action_batch = Tensor(action_batch, dtype=dtypes.float, requires_grad=False)\n",
    "            q_batch = self.critic([state_batch, action_batch])\n",
    "    \n",
    "            value_loss = (q_batch-target_q_batch).pow(2).mean()\n",
    "            self.critic_optim.zero_grad()  # Clears the gradients of all optimized tinygrad.Tensor s.\n",
    "            #print(f'value_loss: {value_loss.numpy()}')\n",
    "            value_loss.backward()  # computes gradients\n",
    "            self.critic_optim.step()  # updates the parameters\n",
    "    \n",
    "            # Actor update\n",
    "    \n",
    "            # self.actor(to_tensor(state_batch)): proto_action_batch\n",
    "            self.actor_optim.zero_grad()\n",
    "            policy_loss = -self.critic([state_batch, self.actor(state_batch)]).mean()\n",
    "            #print(f'policy_loss: {policy_loss.numpy()}')\n",
    "            policy_loss.backward()\n",
    "            self.actor_optim.step()\n",
    "    \n",
    "            # Target update\n",
    "            soft_update(self.actor_target, self.actor, self.tau_update)\n",
    "            soft_update(self.critic_target, self.critic, self.tau_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "825d702d-c186-448b-a2a1-fe2918ed1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    # def _action(self, action):\n",
    "    #     act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "    #     act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "    #     return act_k * action + act_b\n",
    "    #\n",
    "    # def _reverse_action(self, action):\n",
    "    #     act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "    #     act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "    #     return act_k_inv * (action - act_b)\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487d4a0d-6b27-4af7-bb53-f18be8cde122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "def init_parser(alg):\n",
    "\n",
    "    if alg == 'WOLP_DDPG':\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='WOLP_DDPG')\n",
    "\n",
    "        parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor for rewards (default: 0.99)')\n",
    "        parser.add_argument('--max-episode-length', type=int, default=1440, metavar='M', help='maximum length of an episode (default: 1440)')\n",
    "        parser.add_argument('--load', default=False, metavar='L', help='load a trained model')\n",
    "        parser.add_argument('--load-model-dir', default='', metavar='LMD', help='folder to load trained models from')\n",
    "        parser.add_argument('--gpu-ids', type=int, default=[1], nargs='+', help='GPUs to use [-1 CPU only]')\n",
    "        parser.add_argument('--gpu-nums', type=int, default=1, help='#GPUs to use (default: 1)')\n",
    "        parser.add_argument('--max-episode', type=int, default=200000, help='maximum #episode.')\n",
    "        parser.add_argument('--test-episode', type=int, default=1000, help='maximum testing #episode.')\n",
    "        parser.add_argument('--max-actions', default=200000, type=int, help='# max actions')\n",
    "        parser.add_argument('--id', default='0', type=str, help='experiment id')\n",
    "        parser.add_argument('--mode', default='train', type=str, help='support option: train/test')\n",
    "        parser.add_argument('--env', default='Pendulum-v0', type=str, help='Ride sharing')\n",
    "        parser.add_argument('--hidden1', default=256, type=int, help='hidden num of first fully connect layer')\n",
    "        parser.add_argument('--hidden2', default=128, type=int, help='hidden num of second fully connect layer')\n",
    "        parser.add_argument('--c-lr', default=0.001, type=float, help='critic net learning rate')\n",
    "        parser.add_argument('--p-lr', default=0.0001, type=float, help='policy net learning rate (only for DDPG)')\n",
    "        parser.add_argument('--warmup', default=128, type=int, help='time without training but only filling the replay memory')\n",
    "        parser.add_argument('--bsize', default=64, type=int, help='minibatch size')\n",
    "        parser.add_argument('--rmsize', default=6000000, type=int, help='memory size')\n",
    "        parser.add_argument('--window_length', default=1, type=int, help='')\n",
    "        parser.add_argument('--tau-update', default=0.001, type=float, help='moving average for target network')\n",
    "        parser.add_argument('--ou_theta', default=0.15, type=float, help='noise theta')\n",
    "        parser.add_argument('--ou_sigma', default=0.2, type=float, help='noise sigma')\n",
    "        parser.add_argument('--ou_mu', default=0.0, type=float, help='noise mu')\n",
    "        parser.add_argument('--max_episode_length', default=500, type=int, help='')\n",
    "        parser.add_argument('--init_w', default=0.003, type=float, help='')\n",
    "        parser.add_argument('--epsilon', default=80000, type=int, help='Linear decay of exploration policy')\n",
    "        parser.add_argument('--seed', default=-1, type=int, help='')\n",
    "        parser.add_argument('--weight-decay', default=0.0001, type=float, help='weight decay for L2 Regularization loss')\n",
    "        parser.add_argument('--save_per_epochs', default=15, type=int, help='save model every X epochs')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise RuntimeError('undefined algorithm {}'.format(alg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c418b389-a34f-4e7e-b6ac-ecc2b5232def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous? False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m\n\u001b[1;32m     34\u001b[0m     agent_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m: continuous,\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_actions\u001b[39m\u001b[38;5;124m'\u001b[39m: max_actions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m: args,\n\u001b[1;32m     42\u001b[0m     }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContinuous? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontinuous\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mWolpertingerAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43magent_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mWolpertingerAgent.__init__\u001b[0;34m(self, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# according to the papers, it can be scaled to hundreds of millions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mDDPG.__init__\u001b[0;34m(self, args, nb_states, nb_actions)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m Actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target \u001b[38;5;241m=\u001b[39m Actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;241m=\u001b[39m \u001b[43mCritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnet_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target \u001b[38;5;241m=\u001b[39m Critic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[1;32m     30\u001b[0m hard_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor) \u001b[38;5;66;03m# Make sure target is with the same weight\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mCritic.__init__\u001b[0;34m(self, nb_states, nb_actions, hidden1, hidden2, init_w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden1\u001b[38;5;241m+\u001b[39mnb_actions, hidden2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden2, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_w\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mCritic.init_weights\u001b[0;34m(self, init_w)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, init_w):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m     14\u001b[0m         low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m-\u001b[39minit_w), high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(init_w)\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "continuous = None\n",
    "try:\n",
    "    # continuous action\n",
    "    nb_states = env.observation_space.shape[0]\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "    action_high = env.action_space.high\n",
    "    action_low = env.action_space.low\n",
    "    continuous = True\n",
    "    env = NormalizedEnv(env)\n",
    "except IndexError:\n",
    "    # discrete action for 1 dimension\n",
    "    nb_states = env.observation_space.shape[0]\n",
    "    nb_actions = 1  # the dimension of actions, usually it is 1. Depend on the environment.\n",
    "    max_actions = env.action_space.n\n",
    "    continuous = False\n",
    "\n",
    "parser = init_parser('WOLP_DDPG')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if continuous:\n",
    "    agent_args = {\n",
    "        'continuous': continuous,\n",
    "        'max_actions': None,\n",
    "        'action_low': action_low,\n",
    "        'action_high': action_high,\n",
    "        'nb_states': nb_states,\n",
    "        'nb_actions': nb_actions,\n",
    "        'args': args,\n",
    "    }\n",
    "else:\n",
    "    agent_args = {\n",
    "        'continuous': continuous,\n",
    "        'max_actions': max_actions,\n",
    "        'action_low': None,\n",
    "        'action_high': None,\n",
    "        'nb_states': nb_states,\n",
    "        'nb_actions': nb_actions,\n",
    "        'args': args,\n",
    "    }\n",
    "\n",
    "print(f'Continuous? {continuous}')\n",
    "\n",
    "agent = WolpertingerAgent(**agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4b41bb-747c-4759-b784-755a82b9431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override and put a numpy bool in\n",
    "import numpy as np\n",
    "from tinygrad import TinyJit\n",
    "np.bool = np.bool_\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train(continuous, env, agent, max_episode, warmup, save_model_dir, max_episode_length, logger, save_per_epochs):\n",
    "    Tensor.training = True\n",
    "    agent.is_training = True\n",
    "    step = episode = episode_steps = 0\n",
    "    episode_reward = 0.\n",
    "    s_t = None\n",
    "    print(f'max_episode: {max_episode}. save_per_epochs: {save_per_epochs}')\n",
    "    while episode < max_episode:\n",
    "        while True:\n",
    "            if s_t is None:\n",
    "                s_t = env.reset()\n",
    "                if isinstance(s_t, tuple):\n",
    "                    # print(f'Tuple detected for s_t: {s_t}')\n",
    "                    s_t = np.array(s_t[0]).astype(np.float32)\n",
    "                agent.reset(s_t)\n",
    "\n",
    "            # agent pick action ...\n",
    "            # args.warmup: time without training but only filling the memory\n",
    "            if step <= warmup:\n",
    "                action = agent.random_action()\n",
    "            else:\n",
    "                action = agent.select_action(s_t)\n",
    "\n",
    "            #print(f'action: {action}')\n",
    "\n",
    "            # env response with next_observation, reward, terminate_info\n",
    "            if not continuous:\n",
    "                action = action.reshape(1,).astype(int)[0]\n",
    "            s_t1, r_t, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            #print(f's_t1, r_t, done: {s_t1}, {r_t}, {done}')\n",
    "            #print(f's_t1 type: {type(s_t1)}')\n",
    "            s_t1 = np.array(s_t1)\n",
    "\n",
    "            if max_episode_length and episode_steps >= max_episode_length - 1:\n",
    "                done = True\n",
    "\n",
    "            # agent observe and update policy\n",
    "            agent.observe(r_t, s_t1, done)\n",
    "            if step > warmup:\n",
    "                agent.update_policy()\n",
    "\n",
    "            # update\n",
    "            step += 1\n",
    "            episode_steps += 1\n",
    "            episode_reward += r_t\n",
    "            s_t = s_t1\n",
    "            # s_t = deepcopy(s_t1)\n",
    "\n",
    "            if done:  # end of an episode\n",
    "                print(\"Ep:{0} | R:{1:.4f}\".format(episode, episode_reward))\n",
    "                logger.info(\n",
    "                    \"Ep:{0} | R:{1:.4f}\".format(episode, episode_reward)\n",
    "                )\n",
    "\n",
    "                agent.memory.append(\n",
    "                    s_t,\n",
    "                    agent.select_action(s_t),\n",
    "                    0., True\n",
    "                )\n",
    "\n",
    "                # reset\n",
    "                s_t = None\n",
    "                episode_steps =  0\n",
    "                episode_reward = 0.\n",
    "                episode += 1\n",
    "                # break to next episode\n",
    "                break\n",
    "        # [optional] save intermideate model every run through of 32 episodes\n",
    "        if step > warmup and episode > 0 and episode % save_per_epochs == 0:\n",
    "            agent.save_model(save_model_dir)\n",
    "            logger.info(\"### Model Saved before Ep:{0} ###\".format(episode))\n",
    "\n",
    "@TinyJit\n",
    "# @Tensor.test()\n",
    "def test(env, agent, model_path, test_episode, max_episode_length, logger):\n",
    "\n",
    "    agent.load_weights(model_path)\n",
    "    agent.is_training = False\n",
    "    agent.eval()\n",
    "\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0.\n",
    "    s_t = None\n",
    "    for i in range(test_episode):\n",
    "        while True:\n",
    "            if s_t is None:\n",
    "                s_t = env.reset()\n",
    "                agent.reset(s_t)\n",
    "\n",
    "            action = policy(s_t)\n",
    "            s_t, r_t, done, _, _ = env.step(action)\n",
    "            s_t = np.array(s_t)\n",
    "            episode_steps += 1\n",
    "            episode_reward += r_t\n",
    "            if max_episode_length and episode_steps >= max_episode_length - 1:\n",
    "                done = True\n",
    "            if done:  # end of an episode\n",
    "                logger.info(\n",
    "                    \"Ep:{0} | R:{1:.4f}\".format(i+1, episode_reward)\n",
    "                )\n",
    "                s_t = None\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a433587e-280f-47ce-82f9-74ae36a19b57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     l\u001b[38;5;241m.\u001b[39maddHandler(fileHandler)\n\u001b[1;32m     15\u001b[0m     l\u001b[38;5;241m.\u001b[39maddHandler(streamHandler)\n\u001b[0;32m---> 17\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mWolpertingerAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43magent_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m: continuous,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m'\u001b[39m: env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_per_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39msave_per_epochs\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     30\u001b[0m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_args)\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mWolpertingerAgent.__init__\u001b[0;34m(self, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# according to the papers, it can be scaled to hundreds of millions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mDDPG.__init__\u001b[0;34m(self, args, nb_states, nb_actions)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m Actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target \u001b[38;5;241m=\u001b[39m Actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;241m=\u001b[39m \u001b[43mCritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnet_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target \u001b[38;5;241m=\u001b[39m Critic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_cfg)\n\u001b[1;32m     30\u001b[0m hard_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor) \u001b[38;5;66;03m# Make sure target is with the same weight\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mCritic.__init__\u001b[0;34m(self, nb_states, nb_actions, hidden1, hidden2, init_w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden1\u001b[38;5;241m+\u001b[39mnb_actions, hidden2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden2, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_w\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mCritic.init_weights\u001b[0;34m(self, init_w)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, init_w):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m fanin_init(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m     14\u001b[0m         low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m-\u001b[39minit_w), high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(init_w)\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    l.addHandler(streamHandler)\n",
    "\n",
    "agent = WolpertingerAgent(**agent_args)\n",
    "train_args = {\n",
    "    'continuous': continuous,\n",
    "    'env': env,\n",
    "    'agent': agent,\n",
    "    'max_episode': args.max_episode,\n",
    "    'warmup': args.warmup,\n",
    "    'save_model_dir': \"./\",\n",
    "    'max_episode_length': args.max_episode_length,\n",
    "    'logger': logging.getLogger('RS_log'),\n",
    "    'save_per_epochs': args.save_per_epochs\n",
    "}\n",
    "\n",
    "train(**train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc5163-a915-448e-894c-336caf95a745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473cc1f-3c24-42a2-b6de-fbc84ff1d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db1cf2-72a1-4000-863b-6b5f10dadb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e9dec-4cd3-47e9-8571-793a4f18fd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
