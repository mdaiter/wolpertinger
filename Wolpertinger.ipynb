{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "371c9b26-eb59-423b-86cb-38ea1906db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tinygrad\n",
    "from tinygrad import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "3eb2cba9-e51e-4282-a8bb-154dc387970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None) -> Tensor:\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return Tensor.uniform(*size, low=float(-v), high=float(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "27d0282f-dd59-44e1-9380-a648fceb5943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (256, 3) float (<BinaryOps.ADD: 1>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanin_init((256,3), 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "03e42650-0141-4cf3-bf21-5c0c7158d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn, Tensor, dtypes\n",
    "class Actor():\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=256, hidden2=128, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, nb_actions)\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight = fanin_init(self.fc1.weight.size())\n",
    "        self.fc2.weight = fanin_init(self.fc2.weight.size())\n",
    "        self.fc3.weight = Tensor.uniform(\n",
    "            *(self.fc3.weight.size()), \n",
    "            low=float(-init_w), high=float(init_w)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        out = self.fc1(x.cast(dtypes.float)).relu()\n",
    "        out = self.fc2(out).relu()\n",
    "        out = self.fc3(out).softsign()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "d3984d20-bc80-46f4-ac7d-401591299a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (3,) float (<BinaryOps.MUL: 2>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = Actor(3, 3)\n",
    "actor(Tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d91797ba-d74e-47ea-bac2-992afb5ef71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, nb_states, nb_actions, hidden1=319, hidden2=128, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_states, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1+nb_actions, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        #self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight = Tensor.uniform(\n",
    "            *(self.fc3.weight.size()),\n",
    "            low=float(-init_w), high=float(init_w)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, xs:Tensor) -> Tensor:\n",
    "        x, a = xs\n",
    "        x = x.cast(dtypes.float)\n",
    "        a = a.cast(dtypes.float)\n",
    "        out = self.fc1(x).relu()\n",
    "        # concatenate along columns\n",
    "        c_in = out.cat(a, dim=len(a.shape)-1)\n",
    "        out = self.fc2(c_in).relu()\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "fc16e742-863e-4ad5-87ba-fdfefb4401ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (1,) float (<BinaryOps.ADD: 1>, None)> on METAL with grad None>"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic = Critic(2,2)\n",
    "critic(Tensor([[1,2], [2,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "fc140912-826f-4a4b-9195-5f5c2c9d39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MARK - Memory. Should be in a \"memory.py\" file\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py\n",
    "\n",
    "# This is to be understood as a transition: Given `state0`, performing `action`\n",
    "# yields `reward` and results in `state1`, which might be `terminal`.\n",
    "Experience = namedtuple('Experience', 'state0, action, reward, state1, terminal1')\n",
    "\n",
    "\n",
    "def sample_batch_indexes(low, high, size):\n",
    "    if high - low >= size:\n",
    "        # We have enough data. Draw without replacement, that is each index is unique in the\n",
    "        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n",
    "        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n",
    "        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n",
    "        try:\n",
    "            r = xrange(low, high)\n",
    "        except NameError:\n",
    "            r = range(low, high)\n",
    "        batch_idxs = random.sample(r, size)\n",
    "    else:\n",
    "        # Not enough data. Help ourselves with sampling from the range, but the same index\n",
    "        # can occur multiple times. This is not good and should be avoided by picking a\n",
    "        # large enough warm-up phase.\n",
    "        warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
    "        # batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
    "        batch_idxs = np.random.randint(low, high, size=size)\n",
    "    assert len(batch_idxs) == size\n",
    "    return batch_idxs\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        assert isinstance(v, np.ndarray) or isinstance(v, float) or isinstance(v, bool), \"v_type:{}\".format(type(v))\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "\n",
    "def zeroed_observation(observation):\n",
    "    if hasattr(observation, 'shape'):\n",
    "        return np.zeros(observation.shape)\n",
    "    elif hasattr(observation, '__iter__'):\n",
    "        out = []\n",
    "        for x in observation:\n",
    "            out.append(zeroed_observation(x))\n",
    "        return out\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, window_length, ignore_episode_boundaries=False):\n",
    "        self.window_length = window_length\n",
    "        self.ignore_episode_boundaries = ignore_episode_boundaries\n",
    "\n",
    "        self.recent_observations = deque(maxlen=window_length)\n",
    "        self.recent_terminals = deque(maxlen=window_length)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_terminals.append(terminal)\n",
    "\n",
    "    def get_recent_state(self, current_observation):\n",
    "        # This code is slightly complicated by the fact that subsequent observations might be\n",
    "        # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "        # This is probably not that important in practice but it seems cleaner.\n",
    "        state = [current_observation]\n",
    "        idx = len(self.recent_observations) - 1\n",
    "        for offset in range(0, self.window_length - 1):\n",
    "            current_idx = idx - offset\n",
    "            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n",
    "            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                # The previously handled observation was terminal, don't add the current one.\n",
    "                # Otherwise we would leak into a different episode.\n",
    "                break\n",
    "            state.insert(0, self.recent_observations[current_idx])\n",
    "        while len(state) < self.window_length:\n",
    "            state.insert(0, zeroed_observation(state[0]))\n",
    "        return state\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'window_length': self.window_length,\n",
    "            'ignore_episode_boundaries': self.ignore_episode_boundaries,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "class SequentialMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(SequentialMemory, self).__init__(**kwargs)\n",
    "        \n",
    "        self.limit = limit\n",
    "\n",
    "        # Do not use deque to implement the memory. This data structure may seem convenient but\n",
    "        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.terminals = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            # Draw random indexes such that we have at least a single entry before each\n",
    "            # index.\n",
    "            assert self.nb_entries >= 2\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n",
    "        batch_idxs = np.array(batch_idxs) + 1\n",
    "        assert np.min(batch_idxs) >= 1\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        # Create experiences\n",
    "        experiences = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            while terminal0:\n",
    "                # Skip this transition because the environment was reset here. Select a new, random\n",
    "                # transition and use this instead. This may cause the batch to contain the same\n",
    "                # transition twice.\n",
    "                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n",
    "                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n",
    "            assert 1 <= idx < self.nb_entries\n",
    "\n",
    "            # This code is slightly complicated by the fact that subsequent observations might be\n",
    "            # from different episodes. We ensure that an experience never spans multiple episodes.\n",
    "            # This is probably not that important in practice but it seems cleaner.\n",
    "            state0 = [self.observations[idx - 1]]\n",
    "            for offset in range(0, self.window_length - 1):\n",
    "                current_idx = idx - 2 - offset\n",
    "                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state0.insert(0, self.observations[current_idx])\n",
    "            while len(state0) < self.window_length:\n",
    "                state0.insert(0, zeroed_observation(state0[0]))\n",
    "            action = self.actions[idx - 1]\n",
    "            reward = self.rewards[idx - 1]\n",
    "            terminal1 = self.terminals[idx - 1]\n",
    "\n",
    "            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n",
    "            # to the right. Again, we need to be careful to not include an observation from the next\n",
    "            # episode if the last state is terminal.\n",
    "            state1 = [np.copy(x) for x in state0[1:]]\n",
    "            state1.append(self.observations[idx])\n",
    "\n",
    "            assert len(state0) == self.window_length\n",
    "            assert len(state1) == len(state0)\n",
    "            experiences.append(Experience(state0=state0, action=action, reward=reward,\n",
    "                                          state1=state1, terminal1=terminal1))\n",
    "        assert len(experiences) == batch_size\n",
    "        return experiences\n",
    "\n",
    "    def sample_and_split(self, batch_size, batch_idxs=None):\n",
    "        experiences = self.sample(batch_size, batch_idxs)\n",
    "\n",
    "        state0_batch = []\n",
    "        reward_batch = []\n",
    "        action_batch = []\n",
    "        terminal1_batch = []\n",
    "        state1_batch = []\n",
    "        for e in experiences:\n",
    "            state0_batch.append(e.state0)\n",
    "            state1_batch.append(e.state1)\n",
    "            reward_batch.append(e.reward)\n",
    "            action_batch.append(e.action)\n",
    "            terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "        # Prepare and validate parameters.\n",
    "        state0_batch = np.array(state0_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        state1_batch = np.array(state1_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        terminal1_batch = np.array(terminal1_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        reward_batch = np.array(reward_batch).reshape(batch_size,-1).astype(np.float32)\n",
    "        action_batch = np.array(action_batch, dtype=\"object\").reshape(batch_size,-1).astype(np.float32)\n",
    "\n",
    "        return state0_batch, action_batch, reward_batch, state1_batch, terminal1_batch\n",
    "\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        \n",
    "        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n",
    "        # and weather the next state is `terminal` or not.\n",
    "        if training:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "\n",
    "\n",
    "class EpisodeParameterMemory(Memory):\n",
    "    def __init__(self, limit, **kwargs):\n",
    "        super(EpisodeParameterMemory, self).__init__(**kwargs)\n",
    "        self.limit = limit\n",
    "\n",
    "        self.params = RingBuffer(limit)\n",
    "        self.intermediate_rewards = []\n",
    "        self.total_rewards = RingBuffer(limit)\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        batch_params = []\n",
    "        batch_total_rewards = []\n",
    "        for idx in batch_idxs:\n",
    "            batch_params.append(self.params[idx])\n",
    "            batch_total_rewards.append(self.total_rewards[idx])\n",
    "        return batch_params, batch_total_rewards\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        if training:\n",
    "            self.intermediate_rewards.append(reward)\n",
    "\n",
    "    def finalize_episode(self, params):\n",
    "        total_reward = sum(self.intermediate_rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.params.append(params)\n",
    "        self.intermediate_rewards = []\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.total_rewards)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SequentialMemory, self).get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "6122dbb9-993f-4c44-ba7c-e8f08e3f92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# [reference] https://github.com/matthiasplappert/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "312e7756-304f-4a9f-9d80-012663406775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn, Tensor, dtypes, TinyJit\n",
    "from tinygrad.nn.optim import LAMB\n",
    "\n",
    "# might have to change this to \"load_state_dict\" and manually updating state_dict\n",
    "def hard_update(target, source):\n",
    "    for target_tensor, tensor in zip(nn.state.get_parameters(target), nn.state.get_parameters(source)):\n",
    "        tensor.requires_grad = False\n",
    "        target_tensor.replace(tensor)\n",
    "        tensor.requires_grad = True\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, args, nb_states, nb_actions):\n",
    "        if args.seed > 0:\n",
    "            self.seed(args.seed)\n",
    "\n",
    "        self.nb_states =  nb_states\n",
    "        self.nb_actions= nb_actions\n",
    "\n",
    "        net_cfg = {\n",
    "            'hidden1': args.hidden1,\n",
    "            'hidden2': args.hidden2,\n",
    "            'init_w': args.init_w\n",
    "        }\n",
    "        self.actor = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.actor_target = Actor(self.nb_states, self.nb_actions, **net_cfg)\n",
    "\n",
    "        self.critic = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "        self.critic_target = Critic(self.nb_states, self.nb_actions, **net_cfg)\n",
    "\n",
    "        hard_update(self.actor_target, self.actor) # Make sure target is with the same weight\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        print(f'Initialized DDPG with actor parameters: {len(nn.state.get_parameters(self.actor))}, lr={args.p_lr}')\n",
    "        print(f'Initialized DDPG with critic parameters: {len(nn.state.get_parameters(self.critic))}, lr={args.c_lr}')\n",
    "        \n",
    "        self.actor_optim = LAMB(params=nn.state.get_parameters(self.actor), lr=args.p_lr, weight_decay=args.weight_decay, adam=True)\n",
    "        self.critic_optim = LAMB(params=nn.state.get_parameters(self.critic), lr=args.c_lr, weight_decay=args.weight_decay, adam=True)\n",
    "        \n",
    "        #Create replay buffer\n",
    "        self.memory = SequentialMemory(limit=args.rmsize, window_length=args.window_length)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=self.nb_actions,\n",
    "                                                       theta=args.ou_theta, mu=args.ou_mu, sigma=args.ou_sigma)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.batch_size = args.bsize\n",
    "        self.tau_update = args.tau_update\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        # Linear decay rate of exploration policy\n",
    "        self.depsilon = 1.0 / args.epsilon\n",
    "        # initial exploration rate\n",
    "        self.epsilon = 1.0\n",
    "        self.s_t = None # Most recent state\n",
    "        self.a_t = None # Most recent action\n",
    "        self.is_training = True\n",
    "\n",
    "        self.continious_action_space = False\n",
    "\n",
    "    def update_policy(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, r_t, s_t1, done):\n",
    "        if self.is_training:\n",
    "            # print(f'typese of memory: s_t: {self.s_t}, a_t: {self.a_t}')\n",
    "            self.memory.append(self.s_t, self.a_t, r_t, done)\n",
    "            self.s_t = s_t1\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1., 1., self.nb_actions)\n",
    "        # self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        # proto action\n",
    "        if type(s_t) is tuple and s_t[1] == {}:\n",
    "            s_t = s_t[0]\n",
    "        orig_tensor = Tensor([np.array(list(s_t), dtype=np.float32)], dtype=dtypes.float, requires_grad=False)\n",
    "        action = self.actor(orig_tensor).numpy().squeeze(0)\n",
    "        action += self.is_training * max(self.epsilon, 0) * self.random_process.sample()\n",
    "        action = np.clip(action, -1., 1.)\n",
    "\n",
    "        if decay_epsilon:\n",
    "            self.epsilon -= self.depsilon\n",
    "        \n",
    "        # self.a_t = action\n",
    "        return action\n",
    "\n",
    "    def reset(self, s_t):\n",
    "        self.s_t = s_t\n",
    "        self.random_process.reset_states()\n",
    "\n",
    "    def load_weights(self, dir):\n",
    "        if dir is None: return\n",
    "\n",
    "        # load all tensors to CPU\n",
    "        ml = lambda storage, loc: storage\n",
    "\n",
    "        state_dict_actor = nn.state.safe_load(\n",
    "            'output/{}/actor.safetensors'.format(dir)\n",
    "        )\n",
    "        nn.state.load_state_dict(\n",
    "            self.actor, state_dict_actor\n",
    "        )\n",
    "\n",
    "        state_dict_critic = nn.state.safe_load(\n",
    "            'output/{}/critic.safetensors'.format(dir)\n",
    "        )\n",
    "        nn.state.load_state_dict(\n",
    "            self.critic, state_dict_critic\n",
    "        )\n",
    "        print('model weights loaded')\n",
    "\n",
    "\n",
    "    def save_model(self,output):\n",
    "        state_dict_actor = nn.state.get_state_dict(self.actor)\n",
    "        nn.state.safe_save(\n",
    "            state_dict_actor,\n",
    "            '{}/actor.safetensors'.format(output)\n",
    "        )\n",
    "\n",
    "        state_dict_critic = nn.state.get_state_dict(self.critic)\n",
    "        nn.state.safe_save(\n",
    "            state_dict_critic,\n",
    "            '{}/critic.safetensors'.format(output)\n",
    "        )\n",
    "\n",
    "    def seed(self,seed):\n",
    "        Tensor.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "84441f96-d035-40d0-8aec-27d3be5e9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_space.py\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import itertools\n",
    "import pyflann\n",
    "\n",
    "\"\"\"\n",
    "    This class represents a n-dimensional unit cube with a specific number of points embeded.\n",
    "    Points are distributed uniformly in the initialization. A search can be made using the\n",
    "    search_point function that returns the k (given) nearest neighbors of the input point.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Space:\n",
    "\n",
    "    def __init__(self, low, high, points):\n",
    "\n",
    "        self._low = np.array(low)\n",
    "        self._high = np.array(high)\n",
    "        self._range = self._high - self._low\n",
    "        self._dimensions = len(low)\n",
    "        self._space_low = -1\n",
    "        self._space_high = 1\n",
    "        self._k = (self._space_high - self._space_low) / self._range\n",
    "        self.__space = init_uniform_space([self._space_low] * self._dimensions,\n",
    "                                          [self._space_high] * self._dimensions,\n",
    "                                          points).astype(np.float32)\n",
    "        self._flann = pyflann.FLANN()\n",
    "        self.rebuild_flann()\n",
    "\n",
    "    def rebuild_flann(self):\n",
    "        self._index = self._flann.build_index(self.__space, algorithm='kdtree')\n",
    "        print(f'Rebuilding FLANN with: {self._flann}, index: {self._index}')\n",
    "\n",
    "    def search_point(self, point, k):\n",
    "        p_in = point\n",
    "        if not isinstance(point, np.ndarray):\n",
    "            p_in = np.array([p_in]).astype(np.float32)\n",
    "        # p_in = self.import_point(point)\n",
    "        # print(f'p_in.astype(np.float32).shape: {np.shape(p_in.astype(np.float32))}')\n",
    "        if np.shape(p_in) == (1,64,1):\n",
    "            p_in = p_in[0,:,:]\n",
    "        search_res, _ = self._flann.nn_index(p_in.astype(np.float32), k)\n",
    "        knns = self.__space[search_res]\n",
    "        p_out = []\n",
    "        for p in knns:\n",
    "            p_out.append(self.export_point(p))\n",
    "\n",
    "        if k == 1:\n",
    "            p_out = [p_out]\n",
    "        return knns, np.array(p_out)\n",
    "\n",
    "    def import_point(self, point):\n",
    "        return self._space_low + self._k * (point - self._low)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return self._low + (point - self._space_low) / self._k\n",
    "\n",
    "    def get_space(self):\n",
    "        return self.__space\n",
    "\n",
    "    def shape(self):\n",
    "        return self.__space.shape\n",
    "\n",
    "    def get_number_of_actions(self):\n",
    "        return self.shape()[0]\n",
    "\n",
    "\n",
    "class Discrete_space(Space):\n",
    "    \"\"\"\n",
    "        Discrete action space with n actions (the integers in the range [0, n))\n",
    "        1, 2, ..., n-1, n\n",
    "\n",
    "        In gym: 'Discrete' object has no attribute 'high'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):  # n: the number of the discrete actions\n",
    "        super().__init__([0], [n-1], n)\n",
    "\n",
    "    def export_point(self, point):\n",
    "        return np.round(super().export_point(point)).astype(int)\n",
    "\n",
    "\n",
    "def init_uniform_space(low, high, points):\n",
    "    dims = len(low)\n",
    "    # In Discrete situation, the action space is an one dimensional space, i.e., one row\n",
    "    points_in_each_axis = round(points**(1.0 / dims))\n",
    "\n",
    "    axis = []\n",
    "    for i in range(dims):\n",
    "        axis.append(list(np.linspace(low[i], high[i], points_in_each_axis)))\n",
    "\n",
    "    space = []\n",
    "    for _ in itertools.product(*axis):\n",
    "        space.append(list(_))\n",
    "\n",
    "    # space: e.g., [[1], [2], ... ,[n-1]]\n",
    "    return np.array(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "a2c8a305-0ea3-4f6a-9310-d339bfaff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import TinyJit, Tensor\n",
    "\n",
    "# see note above for hard_update\n",
    "def soft_update(target, source, tau_update):\n",
    "    for target_tensor, tensor in zip(nn.state.get_parameters(target), nn.state.get_parameters(source)):\n",
    "        tensor.requires_grad = False\n",
    "        target_tensor.replace(target_tensor * (1.0 - tau_update) + tensor * tau_update)\n",
    "        tensor.requires_grad = True\n",
    "\n",
    "def criterion(input, target):\n",
    "    return ((input-target).pow(2)).mean()\n",
    "\n",
    "class WolpertingerAgent(DDPG):\n",
    "\n",
    "    def __init__(self, continuous, max_actions, action_low, action_high, nb_states, nb_actions, args, k_ratio=0.1):\n",
    "        super().__init__(args, nb_states, nb_actions)\n",
    "        self.experiment = args.id\n",
    "        # according to the papers, it can be scaled to hundreds of millions\n",
    "        if continuous:\n",
    "            self.action_space = Space(action_low, action_high, args.max_actions)\n",
    "            self.k_nearest_neighbors = max(1, int(args.max_actions * k_ratio))\n",
    "        else:\n",
    "            self.action_space = Discrete_space(max_actions)\n",
    "            self.k_nearest_neighbors = max(1, int(max_actions * k_ratio))\n",
    "\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'Wolp3_{}k{}_{}'.format(self.action_space.get_number_of_actions(),\n",
    "                                       self.k_nearest_neighbors, self.experiment)\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.action_space\n",
    "\n",
    "    def wolp_action(self, s_t, proto_action):\n",
    "        # get the proto_action's k nearest neighbors\n",
    "        raw_actions, actions = self.action_space.search_point(proto_action, self.k_nearest_neighbors)\n",
    "\n",
    "        if not isinstance(s_t, np.ndarray):\n",
    "           s_t = s_t.numpy().astype(np.float32)\n",
    "        # make all the state, action pairs for the critic\n",
    "        s_t = np.tile(s_t, [raw_actions.shape[1], 1])\n",
    "\n",
    "        # print(f'raw_actions.shape: {raw_actions.shape}, s_t.shape: {s_t.shape}')\n",
    "        s_t = s_t.reshape(len(raw_actions), raw_actions.shape[1], s_t.shape[-1]) if self.k_nearest_neighbors > 1 \\\n",
    "            else s_t.reshape(raw_actions.shape[0], s_t.shape[-1])\n",
    "        raw_actions = Tensor([raw_actions], dtype=dtypes.float, requires_grad=False)\n",
    "        s_t = Tensor([s_t], dtype=dtypes.float, requires_grad=False)\n",
    "\n",
    "        # evaluate each pair through the critic\n",
    "        actions_evaluation = self.critic([s_t, raw_actions])\n",
    "\n",
    "        # find the index of the pair with the maximum value\n",
    "        max_index = np.argmax(actions_evaluation.numpy(), axis=2)\n",
    "        #print(f'max_index: {max_index}, len(max_index): {len(max_index)}')\n",
    "        #print(f'actions_evaluation.numpy.shape: {np.shape(actions_evaluation.numpy())}, argmax: {np.argmax(actions_evaluation.numpy())}')\n",
    "        max_index = max_index.reshape(len(max_index.flatten()),)\n",
    "\n",
    "        raw_actions = raw_actions.numpy().astype(np.float32)\n",
    "        #print(f'raw_actions.shape: {np.shape(raw_actions)}')\n",
    "        if len(raw_actions.shape) == 4:\n",
    "            raw_actions = raw_actions[0]\n",
    "        # return the best action, i.e., wolpertinger action from the full wolpertinger policy\n",
    "        if self.k_nearest_neighbors > 1:\n",
    "            return raw_actions[[i for i in range(len(raw_actions))], max_index, [0]].reshape(len(raw_actions),1), \\\n",
    "                   actions[[i for i in range(len(actions))], max_index, [0]].reshape(len(actions),1)\n",
    "        else:\n",
    "            return raw_actions[max_index], actions[max_index]\n",
    "\n",
    "    def select_action(self, s_t, decay_epsilon=True):\n",
    "        # taking a continuous action from the actor\n",
    "        proto_action = super().select_action(s_t, decay_epsilon)\n",
    "\n",
    "        #print(f'select_action types: {type(s_t.dtype)}, {type(proto_action.dtype)}')\n",
    "        if type(s_t) is tuple and s_t[1] == {}:\n",
    "            s_t = s_t[0]\n",
    "        raw_wolp_action, wolp_action = self.wolp_action(s_t, proto_action)\n",
    "        assert isinstance(raw_wolp_action, np.ndarray)\n",
    "        self.a_t = raw_wolp_action\n",
    "        # return the best neighbor of the proto action, this is an action for env step\n",
    "        return wolp_action[0]  # [i]\n",
    "\n",
    "    def random_action(self):\n",
    "        proto_action = super().random_action()\n",
    "        raw_action, action = self.action_space.search_point(proto_action, 1)\n",
    "        raw_action = raw_action[0]\n",
    "        action = action[0]\n",
    "        assert isinstance(raw_action, np.ndarray)\n",
    "        self.a_t = raw_action\n",
    "        return action[0] # [i]\n",
    "\n",
    "    def select_target_action(self, s_t):\n",
    "        proto_action = self.actor_target(s_t)\n",
    "        proto_action = proto_action.clamp(-1.0, 1.0).numpy().astype(np.float32)\n",
    "        if type(s_t) is tuple and s_t[1] == {}:\n",
    "            s_t = s_t[0]\n",
    "        raw_wolp_action, wolp_action = self.wolp_action(s_t, proto_action)\n",
    "        return raw_wolp_action\n",
    "\n",
    "    def update_policy(self):\n",
    "        # Sample batch\n",
    "        state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, terminal_batch = self.memory.sample_and_split(self.batch_size)\n",
    "\n",
    "        # Prepare for the target q batch\n",
    "        # the operation below of critic_target does not require backward_P\n",
    "        next_state_batch = Tensor([next_state_batch], dtype=dtypes.float, requires_grad=False)\n",
    "        next_wolp_action_batch = Tensor(self.select_target_action(next_state_batch)[0:1,:,:], dtype=dtypes.float, requires_grad=False)\n",
    "        next_q_values = self.critic_target((\n",
    "            next_state_batch,\n",
    "            next_wolp_action_batch\n",
    "        ))\n",
    "        # but it requires bp in computing gradient of critic loss\n",
    "        # next_q_values.volatile = False\n",
    "\n",
    "        # next_q_values = 0 if is terminal states\n",
    "        target_q_batch = Tensor([reward_batch], dtype=dtypes.float, requires_grad=False) + \\\n",
    "                         self.gamma * \\\n",
    "                         Tensor([terminal_batch.astype(np.float32)], dtype=dtypes.float, requires_grad=False) * \\\n",
    "                         next_q_values\n",
    "\n",
    "        # Critic update\n",
    "        self.critic_optim.zero_grad()  # Clears the gradients of all optimized tinygrad.Tensor s.\n",
    "\n",
    "        state_batch = Tensor([state_batch], dtype=dtypes.float, requires_grad=False)\n",
    "        action_batch = Tensor([action_batch], dtype=dtypes.float, requires_grad=False)\n",
    "        q_batch = self.critic([state_batch, action_batch])\n",
    "\n",
    "        value_loss = criterion(q_batch, target_q_batch)\n",
    "        value_loss.requires_grad = False\n",
    "        value_loss.backward()  # computes gradients\n",
    "        self.critic_optim.step()  # updates the parameters\n",
    "\n",
    "        # Actor update\n",
    "        self.actor_optim.zero_grad()\n",
    "\n",
    "        # self.actor(to_tensor(state_batch)): proto_action_batch\n",
    "        policy_loss = -self.critic([state_batch, self.actor(state_batch)])\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Target update\n",
    "        soft_update(self.actor_target, self.actor, self.tau_update)\n",
    "        soft_update(self.critic_target, self.critic, self.tau_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "825d702d-c186-448b-a2a1-fe2918ed1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    # def _action(self, action):\n",
    "    #     act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "    #     act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "    #     return act_k * action + act_b\n",
    "    #\n",
    "    # def _reverse_action(self, action):\n",
    "    #     act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "    #     act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "    #     return act_k_inv * (action - act_b)\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "487d4a0d-6b27-4af7-bb53-f18be8cde122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "def init_parser(alg):\n",
    "\n",
    "    if alg == 'WOLP_DDPG':\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='WOLP_DDPG')\n",
    "\n",
    "        parser.add_argument('--gamma', type=float, default=0.99, metavar='G', help='discount factor for rewards (default: 0.99)')\n",
    "        parser.add_argument('--max-episode-length', type=int, default=1440, metavar='M', help='maximum length of an episode (default: 1440)')\n",
    "        parser.add_argument('--load', default=False, metavar='L', help='load a trained model')\n",
    "        parser.add_argument('--load-model-dir', default='', metavar='LMD', help='folder to load trained models from')\n",
    "        parser.add_argument('--gpu-ids', type=int, default=[1], nargs='+', help='GPUs to use [-1 CPU only]')\n",
    "        parser.add_argument('--gpu-nums', type=int, default=1, help='#GPUs to use (default: 1)')\n",
    "        parser.add_argument('--max-episode', type=int, default=200000, help='maximum #episode.')\n",
    "        parser.add_argument('--test-episode', type=int, default=1000, help='maximum testing #episode.')\n",
    "        parser.add_argument('--max-actions', default=200000, type=int, help='# max actions')\n",
    "        parser.add_argument('--id', default='0', type=str, help='experiment id')\n",
    "        parser.add_argument('--mode', default='train', type=str, help='support option: train/test')\n",
    "        parser.add_argument('--env', default='Pendulum-v0', type=str, help='Ride sharing')\n",
    "        parser.add_argument('--hidden1', default=256, type=int, help='hidden num of first fully connect layer')\n",
    "        parser.add_argument('--hidden2', default=128, type=int, help='hidden num of second fully connect layer')\n",
    "        parser.add_argument('--c-lr', default=0.001, type=float, help='critic net learning rate')\n",
    "        parser.add_argument('--p-lr', default=0.0001, type=float, help='policy net learning rate (only for DDPG)')\n",
    "        parser.add_argument('--warmup', default=128, type=int, help='time without training but only filling the replay memory')\n",
    "        parser.add_argument('--bsize', default=64, type=int, help='minibatch size')\n",
    "        parser.add_argument('--rmsize', default=6000000, type=int, help='memory size')\n",
    "        parser.add_argument('--window_length', default=1, type=int, help='')\n",
    "        parser.add_argument('--tau-update', default=0.001, type=float, help='moving average for target network')\n",
    "        parser.add_argument('--ou_theta', default=0.15, type=float, help='noise theta')\n",
    "        parser.add_argument('--ou_sigma', default=0.2, type=float, help='noise sigma')\n",
    "        parser.add_argument('--ou_mu', default=0.0, type=float, help='noise mu')\n",
    "        parser.add_argument('--max_episode_length', default=500, type=int, help='')\n",
    "        parser.add_argument('--init_w', default=0.003, type=float, help='')\n",
    "        parser.add_argument('--epsilon', default=80000, type=int, help='Linear decay of exploration policy')\n",
    "        parser.add_argument('--seed', default=-1, type=int, help='')\n",
    "        parser.add_argument('--weight-decay', default=0.0001, type=float, help='weight decay for L2 Regularization loss')\n",
    "        parser.add_argument('--save_per_epochs', default=15, type=int, help='save model every X epochs')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise RuntimeError('undefined algorithm {}'.format(alg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "c418b389-a34f-4e7e-b6ac-ecc2b5232def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DDPG with actor parameters: 6, lr=0.0001\n",
      "Initialized DDPG with critic parameters: 6, lr=0.001\n",
      "Rebuilding FLANN with: <pyflann.index.FLANN object at 0x15d93bbf0>, index: {'algorithm': 'kdtree', 'checks': 32, 'cb_index': 0.5, 'eps': 0.0, 'trees': 1, 'leaf_max_size': 4, 'branching': 32, 'iterations': 5, 'centers_init': 'random', 'target_precision': 0.8999999761581421, 'build_weight': 0.009999999776482582, 'memory_weight': 0.0, 'sample_fraction': 0.10000000149011612, 'log_level': 'warning', 'random_seed': 340544145, 'speedup': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "continuous = None\n",
    "try:\n",
    "    # continuous action\n",
    "    nb_states = env.observation_space.shape[0]\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "    action_high = env.action_space.high\n",
    "    action_low = env.action_space.low\n",
    "    continuous = True\n",
    "    env = NormalizedEnv(env)\n",
    "except IndexError:\n",
    "    # discrete action for 1 dimension\n",
    "    nb_states = env.observation_space.shape[0]\n",
    "    nb_actions = 1  # the dimension of actions, usually it is 1. Depend on the environment.\n",
    "    max_actions = env.action_space.n\n",
    "    continuous = False\n",
    "\n",
    "parser = init_parser('WOLP_DDPG')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if continuous:\n",
    "    agent_args = {\n",
    "        'continuous': continuous,\n",
    "        'max_actions': None,\n",
    "        'action_low': action_low,\n",
    "        'action_high': action_high,\n",
    "        'nb_states': nb_states,\n",
    "        'nb_actions': nb_actions,\n",
    "        'args': args,\n",
    "    }\n",
    "else:\n",
    "    agent_args = {\n",
    "        'continuous': continuous,\n",
    "        'max_actions': max_actions,\n",
    "        'action_low': None,\n",
    "        'action_high': None,\n",
    "        'nb_states': nb_states,\n",
    "        'nb_actions': nb_actions,\n",
    "        'args': args,\n",
    "    }\n",
    "\n",
    "agent = WolpertingerAgent(**agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "3a4b41bb-747c-4759-b784-755a82b9431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override and put a numpy bool in\n",
    "import numpy as np\n",
    "from tinygrad import TinyJit\n",
    "np.bool = np.bool_\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.train()\n",
    "def train(continuous, env, agent, max_episode, warmup, save_model_dir, max_episode_length, logger, save_per_epochs):\n",
    "    agent.is_training = True\n",
    "    step = episode = episode_steps = 0\n",
    "    episode_reward = 0.\n",
    "    s_t = None\n",
    "    print(f'max_episode: {max_episode}. save_per_epochs: {save_per_epochs}')\n",
    "    while episode < max_episode:\n",
    "        while True:\n",
    "            if s_t is None:\n",
    "                s_t = env.reset()\n",
    "                agent.reset(s_t[0])\n",
    "\n",
    "            # agent pick action ...\n",
    "            # args.warmup: time without training but only filling the memory\n",
    "            if step <= warmup:\n",
    "                action = agent.random_action()\n",
    "            else:\n",
    "                action = agent.select_action(s_t)\n",
    "\n",
    "            # env response with next_observation, reward, terminate_info\n",
    "            if not continuous:\n",
    "                action = action.reshape(1,).astype(int)[0]\n",
    "            s_t1, r_t, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            #print(f's_t1, r_t, done: {s_t1}, {r_t}, {done}')\n",
    "            #print(f's_t1 type: {type(s_t1)}')\n",
    "            s_t1 = np.array(s_t1)\n",
    "\n",
    "            if max_episode_length and episode_steps >= max_episode_length - 1:\n",
    "                done = True\n",
    "\n",
    "            # agent observe and update policy\n",
    "            agent.observe(r_t, np.array(s_t1), done)\n",
    "            if step > warmup:\n",
    "                agent.update_policy()\n",
    "\n",
    "            # update\n",
    "            step += 1\n",
    "            episode_steps += 1\n",
    "            episode_reward += r_t\n",
    "            s_t = s_t1\n",
    "            # s_t = deepcopy(s_t1)\n",
    "\n",
    "            if done:  # end of an episode\n",
    "                print(\"Ep:{0} | R:{1:.4f}\".format(episode, episode_reward))\n",
    "                logger.info(\n",
    "                    \"Ep:{0} | R:{1:.4f}\".format(episode, episode_reward)\n",
    "                )\n",
    "\n",
    "                agent.memory.append(\n",
    "                    s_t,\n",
    "                    agent.select_action(s_t),\n",
    "                    0., True\n",
    "                )\n",
    "\n",
    "                # reset\n",
    "                s_t = None\n",
    "                episode_steps =  0\n",
    "                episode_reward = 0.\n",
    "                episode += 1\n",
    "                # break to next episode\n",
    "                break\n",
    "        # [optional] save intermideate model every run through of 32 episodes\n",
    "        if step > warmup and episode > 0 and episode % save_per_epochs == 0:\n",
    "            agent.save_model(save_model_dir)\n",
    "            logger.info(\"### Model Saved before Ep:{0} ###\".format(episode))\n",
    "\n",
    "@TinyJit\n",
    "@Tensor.test()\n",
    "def test(env, agent, model_path, test_episode, max_episode_length, logger):\n",
    "\n",
    "    agent.load_weights(model_path)\n",
    "    agent.is_training = False\n",
    "    agent.eval()\n",
    "\n",
    "    policy = lambda x: agent.select_action(x, decay_epsilon=False)\n",
    "\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0.\n",
    "    s_t = None\n",
    "    for i in range(test_episode):\n",
    "        while True:\n",
    "            if s_t is None:\n",
    "                s_t = env.reset()\n",
    "                agent.reset(s_t)\n",
    "\n",
    "            action = policy(s_t)\n",
    "            s_t, r_t, done, _, _ = env.step(action)\n",
    "            s_t = np.array(s_t)\n",
    "            episode_steps += 1\n",
    "            episode_reward += r_t\n",
    "            if max_episode_length and episode_steps >= max_episode_length - 1:\n",
    "                done = True\n",
    "            if done:  # end of an episode\n",
    "                logger.info(\n",
    "                    \"Ep:{0} | R:{1:.4f}\".format(i+1, episode_reward)\n",
    "                )\n",
    "                s_t = None\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "a433587e-280f-47ce-82f9-74ae36a19b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_episode: 200000. save_per_epochs: 15\n",
      "Ep:0 | R:15.0000\n",
      "Ep:1 | R:21.0000\n",
      "Ep:2 | R:20.0000\n",
      "Ep:3 | R:21.0000\n",
      "Ep:4 | R:28.0000\n",
      "Ep:5 | R:37.0000\n",
      "Ep:6 | R:10.0000\n",
      "Ep:7 | R:10.0000\n",
      "Ep:8 | R:10.0000\n",
      "Ep:9 | R:9.0000\n",
      "Ep:10 | R:10.0000\n",
      "Ep:11 | R:10.0000\n",
      "Ep:12 | R:10.0000\n",
      "Ep:13 | R:10.0000\n",
      "Ep:14 | R:10.0000\n",
      "Ep:15 | R:8.0000\n",
      "Ep:16 | R:10.0000\n",
      "Ep:17 | R:10.0000\n",
      "Ep:18 | R:10.0000\n",
      "Ep:19 | R:8.0000\n",
      "Ep:20 | R:10.0000\n",
      "Ep:21 | R:10.0000\n",
      "Ep:22 | R:10.0000\n",
      "Ep:23 | R:10.0000\n",
      "Ep:24 | R:10.0000\n",
      "Ep:25 | R:10.0000\n",
      "Ep:26 | R:9.0000\n",
      "Ep:27 | R:9.0000\n",
      "Ep:28 | R:9.0000\n",
      "Ep:29 | R:9.0000\n",
      "Ep:30 | R:9.0000\n",
      "Ep:31 | R:10.0000\n",
      "Ep:32 | R:11.0000\n",
      "Ep:33 | R:9.0000\n",
      "Ep:34 | R:9.0000\n",
      "Ep:35 | R:9.0000\n",
      "Ep:36 | R:10.0000\n",
      "Ep:37 | R:9.0000\n",
      "Ep:38 | R:9.0000\n",
      "Ep:39 | R:9.0000\n",
      "Ep:40 | R:9.0000\n",
      "Ep:41 | R:9.0000\n",
      "Ep:42 | R:10.0000\n",
      "Ep:43 | R:9.0000\n",
      "Ep:44 | R:9.0000\n",
      "Ep:45 | R:10.0000\n",
      "Ep:46 | R:8.0000\n",
      "Ep:47 | R:9.0000\n",
      "Ep:48 | R:10.0000\n",
      "Ep:49 | R:10.0000\n",
      "Ep:50 | R:9.0000\n",
      "Ep:51 | R:11.0000\n",
      "Ep:52 | R:10.0000\n",
      "Ep:53 | R:10.0000\n",
      "Ep:54 | R:10.0000\n",
      "Ep:55 | R:10.0000\n",
      "Ep:56 | R:10.0000\n",
      "Ep:57 | R:10.0000\n",
      "Ep:58 | R:8.0000\n",
      "Ep:59 | R:9.0000\n",
      "Ep:60 | R:11.0000\n",
      "Ep:61 | R:9.0000\n",
      "Ep:62 | R:10.0000\n",
      "Ep:63 | R:8.0000\n",
      "Ep:64 | R:9.0000\n",
      "Ep:65 | R:10.0000\n",
      "Ep:66 | R:10.0000\n",
      "Ep:67 | R:11.0000\n",
      "Ep:68 | R:10.0000\n",
      "Ep:69 | R:9.0000\n",
      "Ep:70 | R:9.0000\n",
      "Ep:71 | R:10.0000\n",
      "Ep:72 | R:10.0000\n",
      "Ep:73 | R:9.0000\n",
      "Ep:74 | R:8.0000\n",
      "Ep:75 | R:8.0000\n",
      "Ep:76 | R:10.0000\n",
      "Ep:77 | R:10.0000\n",
      "Ep:78 | R:8.0000\n",
      "Ep:79 | R:8.0000\n",
      "Ep:80 | R:11.0000\n",
      "Ep:81 | R:10.0000\n",
      "Ep:82 | R:9.0000\n",
      "Ep:83 | R:9.0000\n",
      "Ep:84 | R:9.0000\n",
      "Ep:85 | R:9.0000\n",
      "Ep:86 | R:9.0000\n",
      "Ep:87 | R:9.0000\n",
      "Ep:88 | R:9.0000\n",
      "Ep:89 | R:9.0000\n",
      "Ep:90 | R:9.0000\n",
      "Ep:91 | R:10.0000\n",
      "Ep:92 | R:9.0000\n",
      "Ep:93 | R:10.0000\n",
      "Ep:94 | R:9.0000\n",
      "Ep:95 | R:8.0000\n",
      "Ep:96 | R:10.0000\n",
      "Ep:97 | R:9.0000\n",
      "Ep:98 | R:10.0000\n",
      "Ep:99 | R:9.0000\n",
      "Ep:100 | R:9.0000\n",
      "Ep:101 | R:9.0000\n",
      "Ep:102 | R:9.0000\n",
      "Ep:103 | R:9.0000\n",
      "Ep:104 | R:10.0000\n",
      "Ep:105 | R:9.0000\n",
      "Ep:106 | R:10.0000\n",
      "Ep:107 | R:10.0000\n",
      "Ep:108 | R:10.0000\n",
      "Ep:109 | R:10.0000\n",
      "Ep:110 | R:9.0000\n",
      "Ep:111 | R:8.0000\n",
      "Ep:112 | R:9.0000\n",
      "Ep:113 | R:10.0000\n",
      "Ep:114 | R:10.0000\n",
      "Ep:115 | R:9.0000\n",
      "Ep:116 | R:10.0000\n",
      "Ep:117 | R:9.0000\n",
      "Ep:118 | R:8.0000\n",
      "Ep:119 | R:9.0000\n",
      "Ep:120 | R:8.0000\n",
      "Ep:121 | R:10.0000\n",
      "Ep:122 | R:10.0000\n",
      "Ep:123 | R:10.0000\n",
      "Ep:124 | R:10.0000\n",
      "Ep:125 | R:9.0000\n",
      "Ep:126 | R:10.0000\n",
      "Ep:127 | R:10.0000\n",
      "Ep:128 | R:8.0000\n",
      "Ep:129 | R:8.0000\n",
      "Ep:130 | R:9.0000\n",
      "Ep:131 | R:10.0000\n",
      "Ep:132 | R:10.0000\n",
      "Ep:133 | R:10.0000\n",
      "Ep:134 | R:9.0000\n",
      "Ep:135 | R:10.0000\n",
      "Ep:136 | R:8.0000\n",
      "Ep:137 | R:9.0000\n",
      "Ep:138 | R:10.0000\n",
      "Ep:139 | R:9.0000\n",
      "Ep:140 | R:9.0000\n",
      "Ep:141 | R:10.0000\n",
      "Ep:142 | R:10.0000\n",
      "Ep:143 | R:10.0000\n",
      "Ep:144 | R:8.0000\n",
      "Ep:145 | R:10.0000\n",
      "Ep:146 | R:9.0000\n",
      "Ep:147 | R:9.0000\n",
      "Ep:148 | R:11.0000\n",
      "Ep:149 | R:9.0000\n",
      "Ep:150 | R:10.0000\n",
      "Ep:151 | R:9.0000\n",
      "Ep:152 | R:10.0000\n",
      "Ep:153 | R:9.0000\n",
      "Ep:154 | R:9.0000\n",
      "Ep:155 | R:10.0000\n",
      "Ep:156 | R:9.0000\n",
      "Ep:157 | R:9.0000\n",
      "Ep:158 | R:9.0000\n",
      "Ep:159 | R:9.0000\n",
      "Ep:160 | R:10.0000\n",
      "Ep:161 | R:9.0000\n",
      "Ep:162 | R:10.0000\n",
      "Ep:163 | R:10.0000\n",
      "Ep:164 | R:9.0000\n",
      "Ep:165 | R:10.0000\n",
      "Ep:166 | R:9.0000\n",
      "Ep:167 | R:9.0000\n",
      "Ep:168 | R:9.0000\n",
      "Ep:169 | R:10.0000\n",
      "Ep:170 | R:10.0000\n",
      "Ep:171 | R:9.0000\n",
      "Ep:172 | R:10.0000\n",
      "Ep:173 | R:10.0000\n",
      "Ep:174 | R:9.0000\n",
      "Ep:175 | R:10.0000\n",
      "Ep:176 | R:8.0000\n",
      "Ep:177 | R:9.0000\n",
      "Ep:178 | R:11.0000\n",
      "Ep:179 | R:10.0000\n",
      "Ep:180 | R:10.0000\n",
      "Ep:181 | R:10.0000\n",
      "Ep:182 | R:9.0000\n",
      "Ep:183 | R:10.0000\n",
      "Ep:184 | R:10.0000\n",
      "Ep:185 | R:9.0000\n",
      "Ep:186 | R:10.0000\n",
      "Ep:187 | R:10.0000\n",
      "Ep:188 | R:9.0000\n",
      "Ep:189 | R:9.0000\n",
      "Ep:190 | R:9.0000\n",
      "Ep:191 | R:9.0000\n",
      "Ep:192 | R:9.0000\n",
      "Ep:193 | R:9.0000\n",
      "Ep:194 | R:10.0000\n",
      "Ep:195 | R:9.0000\n",
      "Ep:196 | R:10.0000\n",
      "Ep:197 | R:9.0000\n",
      "Ep:198 | R:9.0000\n",
      "Ep:199 | R:10.0000\n",
      "Ep:200 | R:10.0000\n",
      "Ep:201 | R:10.0000\n",
      "Ep:202 | R:10.0000\n",
      "Ep:203 | R:9.0000\n",
      "Ep:204 | R:8.0000\n",
      "Ep:205 | R:10.0000\n",
      "Ep:206 | R:10.0000\n",
      "Ep:207 | R:9.0000\n",
      "Ep:208 | R:10.0000\n",
      "Ep:209 | R:9.0000\n",
      "Ep:210 | R:8.0000\n",
      "Ep:211 | R:9.0000\n",
      "Ep:212 | R:10.0000\n",
      "Ep:213 | R:10.0000\n",
      "Ep:214 | R:9.0000\n",
      "Ep:215 | R:10.0000\n",
      "Ep:216 | R:9.0000\n",
      "Ep:217 | R:10.0000\n",
      "Ep:218 | R:9.0000\n",
      "Ep:219 | R:9.0000\n",
      "Ep:220 | R:9.0000\n",
      "Ep:221 | R:10.0000\n",
      "Ep:222 | R:9.0000\n",
      "Ep:223 | R:10.0000\n",
      "Ep:224 | R:10.0000\n",
      "Ep:225 | R:10.0000\n",
      "Ep:226 | R:8.0000\n",
      "Ep:227 | R:8.0000\n",
      "Ep:228 | R:10.0000\n",
      "Ep:229 | R:9.0000\n",
      "Ep:230 | R:9.0000\n",
      "Ep:231 | R:10.0000\n",
      "Ep:232 | R:8.0000\n",
      "Ep:233 | R:9.0000\n",
      "Ep:234 | R:8.0000\n",
      "Ep:235 | R:10.0000\n",
      "Ep:236 | R:10.0000\n",
      "Ep:237 | R:10.0000\n",
      "Ep:238 | R:9.0000\n",
      "Ep:239 | R:8.0000\n",
      "Ep:240 | R:10.0000\n",
      "Ep:241 | R:10.0000\n",
      "Ep:242 | R:11.0000\n",
      "Ep:243 | R:9.0000\n",
      "Ep:244 | R:10.0000\n",
      "Ep:245 | R:9.0000\n",
      "Ep:246 | R:9.0000\n",
      "Ep:247 | R:8.0000\n",
      "Ep:248 | R:8.0000\n",
      "Ep:249 | R:10.0000\n",
      "Ep:250 | R:10.0000\n",
      "Ep:251 | R:9.0000\n",
      "Ep:252 | R:9.0000\n",
      "Ep:253 | R:10.0000\n",
      "Ep:254 | R:9.0000\n",
      "Ep:255 | R:10.0000\n",
      "Ep:256 | R:9.0000\n",
      "Ep:257 | R:8.0000\n",
      "Ep:258 | R:9.0000\n",
      "Ep:259 | R:10.0000\n",
      "Ep:260 | R:10.0000\n",
      "Ep:261 | R:9.0000\n",
      "Ep:262 | R:9.0000\n",
      "Ep:263 | R:10.0000\n",
      "Ep:264 | R:9.0000\n",
      "Ep:265 | R:9.0000\n",
      "Ep:266 | R:8.0000\n",
      "Ep:267 | R:10.0000\n",
      "Ep:268 | R:10.0000\n",
      "Ep:269 | R:10.0000\n",
      "Ep:270 | R:9.0000\n",
      "Ep:271 | R:9.0000\n",
      "Ep:272 | R:10.0000\n",
      "Ep:273 | R:10.0000\n",
      "Ep:274 | R:10.0000\n",
      "Ep:275 | R:9.0000\n",
      "Ep:276 | R:9.0000\n",
      "Ep:277 | R:9.0000\n",
      "Ep:278 | R:8.0000\n",
      "Ep:279 | R:10.0000\n",
      "Ep:280 | R:10.0000\n",
      "Ep:281 | R:9.0000\n",
      "Ep:282 | R:9.0000\n",
      "Ep:283 | R:9.0000\n",
      "Ep:284 | R:10.0000\n",
      "Ep:285 | R:9.0000\n",
      "Ep:286 | R:8.0000\n",
      "Ep:287 | R:9.0000\n",
      "Ep:288 | R:10.0000\n",
      "Ep:289 | R:10.0000\n",
      "Ep:290 | R:9.0000\n",
      "Ep:291 | R:10.0000\n",
      "Ep:292 | R:10.0000\n",
      "Ep:293 | R:10.0000\n",
      "Ep:294 | R:9.0000\n",
      "Ep:295 | R:10.0000\n",
      "Ep:296 | R:10.0000\n",
      "Ep:297 | R:11.0000\n",
      "Ep:298 | R:10.0000\n",
      "Ep:299 | R:9.0000\n",
      "Ep:300 | R:10.0000\n",
      "Ep:301 | R:10.0000\n",
      "Ep:302 | R:10.0000\n",
      "Ep:303 | R:10.0000\n",
      "Ep:304 | R:10.0000\n",
      "Ep:305 | R:9.0000\n",
      "Ep:306 | R:10.0000\n",
      "Ep:307 | R:9.0000\n",
      "Ep:308 | R:10.0000\n",
      "Ep:309 | R:10.0000\n",
      "Ep:310 | R:11.0000\n",
      "Ep:311 | R:9.0000\n",
      "Ep:312 | R:10.0000\n",
      "Ep:313 | R:10.0000\n",
      "Ep:314 | R:10.0000\n",
      "Ep:315 | R:11.0000\n",
      "Ep:316 | R:10.0000\n",
      "Ep:317 | R:10.0000\n",
      "Ep:318 | R:10.0000\n",
      "Ep:319 | R:9.0000\n",
      "Ep:320 | R:10.0000\n",
      "Ep:321 | R:9.0000\n",
      "Ep:322 | R:10.0000\n",
      "Ep:323 | R:10.0000\n",
      "Ep:324 | R:11.0000\n",
      "Ep:325 | R:9.0000\n",
      "Ep:326 | R:10.0000\n",
      "Ep:327 | R:8.0000\n",
      "Ep:328 | R:9.0000\n",
      "Ep:329 | R:9.0000\n",
      "Ep:330 | R:9.0000\n",
      "Ep:331 | R:9.0000\n",
      "Ep:332 | R:10.0000\n",
      "Ep:333 | R:10.0000\n",
      "Ep:334 | R:8.0000\n",
      "Ep:335 | R:10.0000\n",
      "Ep:336 | R:9.0000\n",
      "Ep:337 | R:10.0000\n",
      "Ep:338 | R:11.0000\n",
      "Ep:339 | R:10.0000\n",
      "Ep:340 | R:9.0000\n",
      "Ep:341 | R:10.0000\n",
      "Ep:342 | R:10.0000\n",
      "Ep:343 | R:9.0000\n",
      "Ep:344 | R:10.0000\n",
      "Ep:345 | R:8.0000\n",
      "Ep:346 | R:10.0000\n",
      "Ep:347 | R:9.0000\n",
      "Ep:348 | R:9.0000\n",
      "Ep:349 | R:8.0000\n",
      "Ep:350 | R:8.0000\n",
      "Ep:351 | R:8.0000\n",
      "Ep:352 | R:9.0000\n",
      "Ep:353 | R:10.0000\n",
      "Ep:354 | R:9.0000\n",
      "Ep:355 | R:9.0000\n",
      "Ep:356 | R:9.0000\n",
      "Ep:357 | R:9.0000\n",
      "Ep:358 | R:10.0000\n",
      "Ep:359 | R:10.0000\n",
      "Ep:360 | R:10.0000\n",
      "Ep:361 | R:9.0000\n",
      "Ep:362 | R:8.0000\n",
      "Ep:363 | R:9.0000\n",
      "Ep:364 | R:9.0000\n",
      "Ep:365 | R:9.0000\n",
      "Ep:366 | R:10.0000\n",
      "Ep:367 | R:10.0000\n",
      "Ep:368 | R:9.0000\n",
      "Ep:369 | R:10.0000\n",
      "Ep:370 | R:8.0000\n",
      "Ep:371 | R:10.0000\n",
      "Ep:372 | R:10.0000\n",
      "Ep:373 | R:8.0000\n",
      "Ep:374 | R:8.0000\n",
      "Ep:375 | R:9.0000\n",
      "Ep:376 | R:9.0000\n",
      "Ep:377 | R:10.0000\n",
      "Ep:378 | R:9.0000\n",
      "Ep:379 | R:10.0000\n",
      "Ep:380 | R:9.0000\n",
      "Ep:381 | R:11.0000\n",
      "Ep:382 | R:10.0000\n",
      "Ep:383 | R:9.0000\n",
      "Ep:384 | R:10.0000\n",
      "Ep:385 | R:8.0000\n",
      "Ep:386 | R:9.0000\n",
      "Ep:387 | R:10.0000\n",
      "Ep:388 | R:9.0000\n",
      "Ep:389 | R:10.0000\n",
      "Ep:390 | R:9.0000\n",
      "Ep:391 | R:9.0000\n",
      "Ep:392 | R:9.0000\n",
      "Ep:393 | R:10.0000\n",
      "Ep:394 | R:9.0000\n",
      "Ep:395 | R:10.0000\n",
      "Ep:396 | R:10.0000\n",
      "Ep:397 | R:9.0000\n",
      "Ep:398 | R:11.0000\n",
      "Ep:399 | R:9.0000\n",
      "Ep:400 | R:9.0000\n",
      "Ep:401 | R:8.0000\n",
      "Ep:402 | R:9.0000\n",
      "Ep:403 | R:9.0000\n",
      "Ep:404 | R:10.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[489], line 29\u001b[0m\n\u001b[1;32m     15\u001b[0m     l\u001b[38;5;241m.\u001b[39maddHandler(streamHandler)\n\u001b[1;32m     17\u001b[0m train_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m: continuous,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m'\u001b[39m: env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_per_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39msave_per_epochs\n\u001b[1;32m     27\u001b[0m }\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/jit.py:227\u001b[0m, in \u001b[0;36mTinyJit.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfxn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    226\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Context(BEAM\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIGNORE_JIT_FIRST_BEAM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m BEAM\u001b[38;5;241m.\u001b[39mvalue):\n\u001b[0;32m--> 227\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfxn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(params\u001b[38;5;241m:=\u001b[39mget_parameters(ret)): Tensor\u001b[38;5;241m.\u001b[39mrealize(params[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    230\u001b[0m   \u001b[38;5;66;03m# jit capture\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[488], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(continuous, env, agent, max_episode, warmup, save_model_dir, max_episode_length, logger, save_per_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m agent\u001b[38;5;241m.\u001b[39mobserve(r_t, np\u001b[38;5;241m.\u001b[39marray(s_t1), done)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m warmup:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[1;32m     45\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[481], line 107\u001b[0m, in \u001b[0;36mWolpertingerAgent.update_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Prepare for the target q batch\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# the operation below of critic_target does not require backward_P\u001b[39;00m\n\u001b[1;32m    106\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m Tensor([next_state_batch], dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mfloat, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 107\u001b[0m next_wolp_action_batch \u001b[38;5;241m=\u001b[39m Tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_target_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m,:,:], dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mfloat, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target((\n\u001b[1;32m    109\u001b[0m     next_state_batch,\n\u001b[1;32m    110\u001b[0m     next_wolp_action_batch\n\u001b[1;32m    111\u001b[0m ))\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# but it requires bp in computing gradient of critic loss\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# next_q_values.volatile = False\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# next_q_values = 0 if is terminal states\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[481], line 93\u001b[0m, in \u001b[0;36mWolpertingerAgent.select_target_action\u001b[0;34m(self, s_t)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_target_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, s_t):\n\u001b[1;32m     92\u001b[0m     proto_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target(s_t)\n\u001b[0;32m---> 93\u001b[0m     proto_action \u001b[38;5;241m=\u001b[39m \u001b[43mproto_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(s_t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s_t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m {}:\n\u001b[1;32m     95\u001b[0m         s_t \u001b[38;5;241m=\u001b[39m s_t[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:305\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _to_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno np dtype for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m all_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno data if shape is symbolic, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m_to_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:249\u001b[0m, in \u001b[0;36mTensor._data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mbytearray\u001b[39m(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# NOTE: this realizes on the object from as_buffer being a Python object\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m cpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCLANG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m buf \u001b[38;5;241m=\u001b[39m cast(Buffer, cast(LazyBuffer, cpu\u001b[38;5;241m.\u001b[39mlazydata)\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mrealized)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLANG\u001b[39m\u001b[38;5;124m\"\u001b[39m: buf\u001b[38;5;241m.\u001b[39moptions \u001b[38;5;241m=\u001b[39m BufferOptions(nolru\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:3380\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3382\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3383\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/tensor.py:208\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self, do_update_stats, *lst)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrealize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mlst:Tensor, do_update_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Triggers the computation needed to create these Tensor(s).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m   \u001b[43mrun_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_with_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_update_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_update_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:223\u001b[0m, in \u001b[0;36mrun_schedule\u001b[0;34m(schedule, var_vals, do_update_stats)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m lower_schedule(schedule):\n\u001b[1;32m    222\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(capturing) \u001b[38;5;129;01mand\u001b[39;00m CAPTURING: capturing[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39madd(ei)\n\u001b[0;32m--> 223\u001b[0m   \u001b[43mei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_update_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_update_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:173\u001b[0m, in \u001b[0;36mExecItem.run\u001b[0;34m(self, var_vals, wait, jit, do_update_stats)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_vals:Optional[Dict[Variable, \u001b[38;5;28mint\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, jit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, do_update_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    172\u001b[0m   bufs \u001b[38;5;241m=\u001b[39m [cast(Buffer, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbufs] \u001b[38;5;28;01mif\u001b[39;00m jit \u001b[38;5;28;01melse\u001b[39;00m [cast(Buffer, x)\u001b[38;5;241m.\u001b[39mensure_allocated() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbufs]\n\u001b[0;32m--> 173\u001b[0m   et \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvar_vals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m do_update_stats:\n\u001b[1;32m    175\u001b[0m     GlobalCounters\u001b[38;5;241m.\u001b[39mkernel_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:139\u001b[0m, in \u001b[0;36mBufferCopy.__call__\u001b[0;34m(self, rawbufs, var_vals, wait)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dest\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m src\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mand\u001b[39;00m dest\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m src\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer copy mismatch, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    141\u001b[0m   Device[dest\u001b[38;5;241m.\u001b[39mdevice]\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/engine/realize.py:134\u001b[0m, in \u001b[0;36mBufferCopy.copy\u001b[0;34m(self, dest, src)\u001b[0m\n\u001b[1;32m    132\u001b[0m   src\u001b[38;5;241m.\u001b[39mallocator\u001b[38;5;241m.\u001b[39mcopyout(dest\u001b[38;5;241m.\u001b[39mallocator\u001b[38;5;241m.\u001b[39mas_buffer(dest\u001b[38;5;241m.\u001b[39m_buf), src\u001b[38;5;241m.\u001b[39m_buf)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m   dest\u001b[38;5;241m.\u001b[39mcopyin(\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_zero_copy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/device.py:111\u001b[0m, in \u001b[0;36mBuffer.as_buffer\u001b[0;34m(self, allow_zero_copy, force_zero_copy)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m, allow_zero_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, force_zero_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m:\n\u001b[1;32m    110\u001b[0m   \u001b[38;5;66;03m# zero copy with as_buffer (disabled by default due to use after free)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (force_zero_copy \u001b[38;5;129;01mor\u001b[39;00m allow_zero_copy) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallocator, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_zero_copy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce zero copy was passed, but copy is required\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopyout(\u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mbytearray\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbytes)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:96\u001b[0m, in \u001b[0;36mMetalAllocator.as_buffer\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m, src:MetalBuffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mbuf\u001b[38;5;241m.\u001b[39mcontents()\u001b[38;5;241m.\u001b[39mas_buffer(src\u001b[38;5;241m.\u001b[39moffset\u001b[38;5;241m+\u001b[39msrc\u001b[38;5;241m.\u001b[39msize)[src\u001b[38;5;241m.\u001b[39moffset:]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:118\u001b[0m, in \u001b[0;36mMetalDevice.synchronize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m cbuf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtl_buffers_in_flight: \u001b[43mwait_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmv_in_metal\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    120\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmtl_buffers_in_flight\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/tinygrad/runtime/ops_metal.py:10\u001b[0m, in \u001b[0;36mwait_check\u001b[0;34m(cbuf)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_check\u001b[39m(cbuf: Any):\n\u001b[0;32m---> 10\u001b[0m   \u001b[43mcbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitUntilCompleted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (error \u001b[38;5;241m:=\u001b[39m cbuf\u001b[38;5;241m.\u001b[39merror()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    l.addHandler(streamHandler)\n",
    "\n",
    "train_args = {\n",
    "    'continuous': continuous,\n",
    "    'env': env,\n",
    "    'agent': agent,\n",
    "    'max_episode': args.max_episode,\n",
    "    'warmup': args.warmup,\n",
    "    'save_model_dir': \"./\",\n",
    "    'max_episode_length': args.max_episode_length,\n",
    "    'logger': logging.getLogger('RS_log'),\n",
    "    'save_per_epochs': args.save_per_epochs\n",
    "}\n",
    "\n",
    "train(**train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc5163-a915-448e-894c-336caf95a745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473cc1f-3c24-42a2-b6de-fbc84ff1d6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db1cf2-72a1-4000-863b-6b5f10dadb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e9dec-4cd3-47e9-8571-793a4f18fd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
